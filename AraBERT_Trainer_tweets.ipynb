{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraBERT-Trainer-tweets",
      "provenance": [],
      "collapsed_sections": [
        "5EZF6bJbtq5X",
        "MA5607R9jU3a",
        "AANFYzSruvH9"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fouad89/arabicNLP/blob/main/AraBERT_Trainer_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mZ8KYblg-g"
      },
      "source": [
        "#installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tlwQ8si_FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df3bc3-76fe-4ec0-b64a-edede7b9db62"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Fri Jun 10 09:39:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y024z5AnlTLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dbfe1b-7f5a-4c12-ee80-73ce634a7abb"
      },
      "source": [
        "!pip install optuna==2.3.0\n",
        "!pip install transformers==4.2.1\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==2.3.0\n",
            "  Downloading optuna-2.3.0.tar.gz (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 5.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.1.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.36)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (21.3)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.1)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (4.64.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.3.0) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (4.11.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (1.1.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.3.0) (5.7.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (3.13)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 65.3 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (3.3.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (4.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.3.0) (2.0.1)\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-py3-none-any.whl size=359772 sha256=889bcd834901246b0a43e5e4c21a0a1058611f9eecd4821b9cc4d774c5c34707\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/61/9e/955ab1890f6cab231b1d756db63f36c711968a324296e0b649\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=1d3255bc21b22e7b7ace577dc14ab394cfec5f70070474f50035dcb7da0fc5b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.0 alembic-1.8.0 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 optuna-2.3.0 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.2.1\n",
            "  Downloading transformers-4.2.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.11.4)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.1) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ff3576e3e390aa86184d79a8c32949f4fd10f347145c711df5e12d92e6ad816a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farasapy) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.14\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 569 (delta 26), reused 24 (delta 23), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (569/569), 9.12 MiB | 32.44 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlMoq-hmZ61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88111630-5088-44c5-b00f-5f8abaef44d7"
      },
      "source": [
        "# !git clone https://github.com/elnagara/HARD-Arabic-Dataset\n",
        "# !git clone https://github.com/mahmoudnabil/ASTD\n",
        "# !git clone https://github.com/nora-twairesh/AraSenti\n",
        "# !git clone https://github.com/mohamedadaly/LABR\n",
        "# !wget http://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip\n",
        "# !unzip ArSAS.zip\n",
        "# !unrar x '/content/HARD-Arabic-Dataset/data/unbalanced-reviews.rar'\n",
        "# !unzip '/content/HARD-Arabic-Dataset/data/balanced-reviews.zip'\n",
        "!git clone https://github.com/fouad89/arabicNLP"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabicNLP'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 34 (delta 14), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krBvefg6l6vv"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir train"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVay9KamnC3I"
      },
      "source": [
        "#Creating training datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr84ozGinCFh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "all_datasets= []"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PhWP2JzrEci"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WYy5ikAs7l3"
      },
      "source": [
        "DATA_COLUMN = \"text\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joWPYWGMqLau"
      },
      "source": [
        "##HARD - Balanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6oL3qkXmOgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0e848b-0a45-4019-a02b-03094fe8d6cf"
      },
      "source": [
        "df_HARD = pd.read_csv(\"arabicNLP/data/Tweets.txt\", sep=\"\\t\", header=None)\n",
        "\n",
        "# df_HARD = df_HARD[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
        "df_HARD.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "print(df_HARD[LABEL_COLUMN].value_counts())\n",
        "# # code rating as +ve if > 3, -ve if less, no 3s in dataset\n",
        "\n",
        "# hard_map = {\n",
        "#     5: 'POS',\n",
        "#     4: 'POS',\n",
        "#     2: 'NEG',\n",
        "#     1: 'NEG'\n",
        "# }\n",
        "label_encoder = LabelEncoder()\n",
        "# df_HARD['label'] = label_encoder.fit_transform(df_HARD['label'])\n",
        "\n",
        "# mapping display\n",
        "# encoder_map = dict(zip( label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
        "label_list_HARD = ['NEG', 'POS']\n",
        "\n",
        "df_HARD = df_HARD[df_HARD.label.isin(label_list_HARD)]\n",
        "\n",
        "\n",
        "# print classes\n",
        "# print(encoder_map)\n",
        "# print(label_encoder.classes_ )\n",
        "# df_HARD[LABEL_COLUMN] = df_HARD[LABEL_COLUMN].apply(lambda x: hard_map[x])\n",
        "train_HARD, test_HARD = train_test_split(df_HARD, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert to dataset\n",
        "data_Hard = Dataset(\"HARD\", train_HARD, test_HARD, label_list_HARD)\n",
        "all_datasets.append(data_Hard)\n",
        "# df_HARD"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OBJ        6470\n",
            "NEG        1642\n",
            "NEUTRAL     805\n",
            "POS         777\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_Hard.train.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-yj4SGABtSD",
        "outputId": "c5b274e4-9f03-4a8f-e03f-71083a7bf3cb"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NEG    1322\n",
              "POS     613\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcwdslw7v0Q8"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUn2RB6Bvrxj"
      },
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "import optuna "
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfNKr05tv7cA"
      },
      "source": [
        "logging.basicConfig(level=logging.WARNING)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4SGYoB2EDJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe81b444-2c64-4ab1-e235-c2084e580740"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HARD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Lma6tT5zJi"
      },
      "source": [
        "You can choose which model, and dataset from here along with the max sentence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzeVFoz1wDYf"
      },
      "source": [
        "dataset_name = 'HARD'\n",
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "task_name = 'classification'\n",
        "max_len = 256"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieCOj90aw8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa354851-103c-48ec-bbd5-b7f26eb8f310"
      },
      "source": [
        "for d in all_datasets:\n",
        "  if d.name==dataset_name:\n",
        "    selected_dataset = d\n",
        "    print('Dataset found')\n",
        "    break"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt_lGy85zuca"
      },
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name.split(\"/\")[-1])\n",
        "\n",
        "selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))\n",
        "selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))  "
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_dataset.train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "B4eah5PoIG1X",
        "outputId": "beb40a8d-4d3f-45ff-b38c-5fb9f83b89d6"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text label\n",
              "7233  أيها الاخوان الا تذكرون 52 شهيدا قتلتهم شرطة م...   NEG\n",
              "7006  يجب على كل ابناء # الزمالك مساندة ميدو ودعمه ب...   POS\n",
              "1855  نسأل الله أن يحفظ شيخنا من كل مكروه ويمن عليه ...   POS\n",
              "6113  اللى بيقول ان البلد دى بلدنا لما حد بيقوله يا ...   NEG\n",
              "7140  # حركة _ كايدينهم # مصر # قول _ كلمه _ للسيسي ...   POS\n",
              "...                                                 ...   ...\n",
              "7130  المادة 139 : 16 معترض وبعد نقاش 8 دقائق يعاد ا...   NEG\n",
              "4785  # عمر _ جابر مننا هيه هيه # زملكاوى زيينا هيه ...   POS\n",
              "4978  تحالف الاخوان : الانتخابات الرئاسية باطلة لان ...   NEG\n",
              "5745                      نشكرك تكلمتي بلسانا جميعا : )   POS\n",
              "3561  الانقلاب يقتل السوريين في البحر قبل ان يدخل عل...   NEG\n",
              "\n",
              "[1935 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1e4bd2f-dbdf-47bc-8848-537bf27cf686\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7233</th>\n",
              "      <td>أيها الاخوان الا تذكرون 52 شهيدا قتلتهم شرطة م...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7006</th>\n",
              "      <td>يجب على كل ابناء # الزمالك مساندة ميدو ودعمه ب...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1855</th>\n",
              "      <td>نسأل الله أن يحفظ شيخنا من كل مكروه ويمن عليه ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6113</th>\n",
              "      <td>اللى بيقول ان البلد دى بلدنا لما حد بيقوله يا ...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7140</th>\n",
              "      <td># حركة _ كايدينهم # مصر # قول _ كلمه _ للسيسي ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7130</th>\n",
              "      <td>المادة 139 : 16 معترض وبعد نقاش 8 دقائق يعاد ا...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4785</th>\n",
              "      <td># عمر _ جابر مننا هيه هيه # زملكاوى زيينا هيه ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4978</th>\n",
              "      <td>تحالف الاخوان : الانتخابات الرئاسية باطلة لان ...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5745</th>\n",
              "      <td>نشكرك تكلمتي بلسانا جميعا : )</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3561</th>\n",
              "      <td>الانقلاب يقتل السوريين في البحر قبل ان يدخل عل...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1935 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1e4bd2f-dbdf-47bc-8848-537bf27cf686')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1e4bd2f-dbdf-47bc-8848-537bf27cf686 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1e4bd2f-dbdf-47bc-8848-537bf27cf686');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YS7XI2bZTyz"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        \n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )     \n",
        "    \n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \n",
        "      \n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciZOFz-amkV",
        "outputId": "9b9aa002-f45f-48ff-c3ef-5defc2c5da0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "train_dataset = BERTDataset(selected_dataset.train[DATA_COLUMN].to_list(),selected_dataset.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
        "test_dataset = BERTDataset(selected_dataset.test[DATA_COLUMN].to_list(),selected_dataset.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEG': 0, 'POS': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsOI2MlxHNLN",
        "outputId": "f3c701b0-7aba-4312-e6c2-bb94f895494a"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NEG': 0, 'POS': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt7l0IxjbmNu"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYU6G4vWc5nz"
      },
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  #print(classification_report(p.label_ids,preds))\n",
        "  #print(confusion_matrix(p.label_ids,preds))\n",
        "\n",
        "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {\n",
        "      'macro_f1' : macro_f1,\n",
        "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
        "      'macro_precision': macro_precision,\n",
        "      'macro_recall': macro_recall,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTmvFEs41WkV"
      },
      "source": [
        "#Regular Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_oGjIC-7Vow"
      },
      "source": [
        "This paert allows you to do a regular training with no hyper parameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xjs-X14uc0",
        "outputId": "0b8634cf-29f8-4c59-ab8e-64712f6425cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 16\n",
        "training_args.per_device_eval_batch_size = 16\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= 2\n",
        "\n",
        "\n",
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)\n",
        "#Warmup_ratio\n",
        "warmup_ratio = 0.1\n",
        "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
        "\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
        "training_args.seed = 42\n",
        "training_args.disable_tqdm = False\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro5BW5ak4uc1",
        "outputId": "3af12a52-33df-497d-a792-b463b4ee6f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx336O3J2SdQ",
        "outputId": "496016e1-ec81-403b-a4cd-8a4a44ca6632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 01:44, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Macro F1 Pos Neg</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.300752</td>\n",
              "      <td>0.859564</td>\n",
              "      <td>0.859564</td>\n",
              "      <td>0.859039</td>\n",
              "      <td>0.860099</td>\n",
              "      <td>0.873967</td>\n",
              "      <td>3.992800</td>\n",
              "      <td>121.218000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.307981</td>\n",
              "      <td>0.872616</td>\n",
              "      <td>0.872616</td>\n",
              "      <td>0.874363</td>\n",
              "      <td>0.870960</td>\n",
              "      <td>0.886364</td>\n",
              "      <td>3.983400</td>\n",
              "      <td>121.504000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=120, training_loss=0.3559512456258138, metrics={'train_runtime': 105.4223, 'train_samples_per_second': 1.138, 'total_flos': 800526750888960, 'epoch': 1.99})"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Seyz8Yaj2ZCK"
      },
      "source": [
        "trainer.save_model(\"SOME_PATH\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}